{"cells":[{"cell_type":"markdown","id":"7f8849bd","metadata":{"id":"7f8849bd"},"source":["# Data Processing"]},{"cell_type":"code","execution_count":null,"id":"a654961c","metadata":{"id":"a654961c"},"outputs":[],"source":["from IPython.display import display, HTML\n","display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns"]},{"cell_type":"code","source":["np.seterr(divide = 'ignore') "],"metadata":{"id":"MFtozSKAbZwu"},"id":"MFtozSKAbZwu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"V5b7fpXLlH0J"},"id":"V5b7fpXLlH0J","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"233f343b","metadata":{"id":"233f343b"},"outputs":[],"source":["data_train_X = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_train_X')\n","data_train_y = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_train_y')\n","data_test_X = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_test_X')\n","data_test_y = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_test_y')\n","working_site_data = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/DATA/working_site_metadata.csv')"]},{"cell_type":"code","execution_count":null,"id":"6fc5ee5a","metadata":{"id":"6fc5ee5a"},"outputs":[],"source":["data_train_X.shape"]},{"cell_type":"markdown","id":"68169a17","metadata":{"id":"68169a17"},"source":["## This is third version of dataset processing. \n","\n","New series were created:\n","* log_max_avg_diff_genergy = log(max_genergy - avg_genergy)\n","* log_max_avg_diff_diff_genergy = log(max_difference_in_genergy - avg_difference_in_genergy)\n","* log_max_avg_diff_gactivity = log(max_gactivity - avg_gactivity)\n","* log_max_avg_diff_diff_gactivity = log(max_difference_in_gactivity - avg_difference_in_gactivity)\n","* log_ave_genergy = log(sum / count)\n","---\n","The main idea was to extract features like:\n","* minimum\n","* maximum\n","* std\n","* quantiles: 0,25 ; 0.5 ; 0.75\n","* information if there is non-zero value in the test series (True/ False),\n","* number of positive values in a series,\n","* hours from last non-zero measurement,\n","* number of times a series increased in comparison to the previous hour's recording,\n","These statistic were computed over the last 4, 8 and 24 hours. \n","\n","Above extraction methodology affect following time series:\n","* count_e, \n","* sum_e, \n","* number_of_rock_bursts\n","* number_of_destressing_blasts\n","* avg_gactivity\n","* avg_genergy\n","----\n","For series:\n","* avg_difference_in_gactivity \n","* avg_difference_in_genergy \n","\n","computed:\n","* max abs value \n","* number of positive values in a series,\n","over last 4, 8, 24 hours\n","---\n","Categorical variables:\n","* latest_seismic_assesment\n","* latest_seismoacoustic_assessment \n","* latest_comprehensive_assessment\n","* latetest_hazards_assessment\n","* mining_hazard_assessment (from working_site_metada additional dataset)\n","were encoded with one-hot encoding.\n","----\n","---------------------------------------------\n","From extra dataset following features were used:\n","* mining_hazard_assessment (as mentioned above)\n","* main_working_height "]},{"cell_type":"code","source":["def ts_col_names(first_ts_col, last_ts_col):\n","  return list(data_train_X.iloc[:, first_ts_col: last_ts_col].columns)"],"metadata":{"id":"nby0Y95CZ92n"},"id":"nby0Y95CZ92n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#count_eX\n","counts_e2 = ts_col_names(13, 37)\n","counts_e3 = ts_col_names(37, 61)\n","counts_e4 = ts_col_names(61, 85)\n","counts_e5 = ts_col_names(85, 109)\n","counts_e6plus = ts_col_names(109, 133)\n","#sum_eX\n","sum_e2 = ts_col_names(133, 157)\n","sum_e3 = ts_col_names(157, 181)\n","sum_e4 = ts_col_names(181, 205)\n","sum_e5 = ts_col_names(205, 229)\n","sum_e6plus = ts_col_names(229, 253)\n","#total_number_of_bumps\n","total_number_of_bumps = ts_col_names(253, 277)\n","#number_of_rock_bursts\n","num_rock_bursts = ts_col_names(277,301)\n","#number_of_destressing_blasts\n","num_des_blasts = ts_col_names(301, 325)\n","#highest_bump_energy\n","h_bump_energy = ts_col_names(325, 349)\n","#other time series\n","max_gactivities = ts_col_names(349, 373)\n","max_genergies = ts_col_names(373, 397)\n","avg_gactivities = ts_col_names(397, 421)\n","avg_genergies = ts_col_names(421, 445)\n","max_diff_gactivities = ts_col_names(445, 469)\n","max_diff_genergies = ts_col_names(469, 493)\n","avg_diff_gactivities = ts_col_names(493, 517)\n","avg_diff_genergies = ts_col_names(517, 541)"],"metadata":{"id":"abksGBSpZ_d0"},"id":"abksGBSpZ_d0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["ts_to_agg = [counts_e2, counts_e3, counts_e4, counts_e5, counts_e6plus, \n","             sum_e2, sum_e3, sum_e4, sum_e5, sum_e6plus,\n","             num_rock_bursts, num_des_blasts,\n","             max_gactivities, max_genergies, avg_gactivities, avg_genergies,\n","             max_diff_gactivities, max_diff_genergies, \n","             avg_diff_gactivities, avg_diff_genergies]"],"metadata":{"id":"_qYDE7dn7boc"},"id":"_qYDE7dn7boc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New series deriving"],"metadata":{"id":"cFMkjWrQXRVT"},"id":"cFMkjWrQXRVT"},{"cell_type":"markdown","source":["## log_max_avg series\n","* log_max_avg_diff_genergy = log(max_genergy - avg_genergy)\n","* log_max_avg_diff_diff_genergy = log(max_difference_in_genergy - avg_difference_in_genergy)\n","* log_max_avg_diff_gactivity = log(max_gactivity - avg_gactivity)\n","* log_max_avg_diff_diff_gactivity = log(max_difference_in_gactivity - avg_difference_in_gactivity)"],"metadata":{"id":"KFteDtL7ZZQP"},"id":"KFteDtL7ZZQP"},{"cell_type":"code","source":["def log_max_avg(df, max_feature, avg_feature, save_col_names=0):\n","\n","  new_col_names = []\n","\n","  for i in range (len(max_feature)):\n","\n","    col_name = ('log__'+df[max_feature].columns[i][:-2] + \n","                '-' + \n","                df[avg_feature].columns[i][:-2]+'_'+str(i+1))\n","    \n","    df[col_name] = np.log(df[max_feature[i]] - df[avg_feature[i]])\n","    df[col_name].replace([np.inf, -np.inf], 0, inplace=True)\n","\n","\n","    if save_col_names:\n","      new_col_names.append(col_name)\n","\n","  return df, new_col_names"],"metadata":{"id":"62xtjRHoZb_6"},"id":"62xtjRHoZb_6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#new series in train dataset\n","data_train_X, log_max_avg_genergies = log_max_avg(\n","    data_train_X, max_genergies, \n","    avg_genergies, 1)\n","data_train_X, log_diff_max_avg_genergies = log_max_avg(\n","    data_train_X, max_diff_genergies,\n","     avg_diff_genergies, 1)\n","data_train_X, log_max_avg_gactivities = log_max_avg(\n","    data_train_X, max_gactivities,\n","     avg_gactivities, 1)\n","data_train_X, log_diff_max_avg_gactivities = log_max_avg(\n","    data_train_X, max_diff_gactivities,\n","     avg_diff_gactivities, 1)"],"metadata":{"id":"RxnaoJdgtT43"},"id":"RxnaoJdgtT43","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#same new series in test dataset\n","log_max_avg(data_test_X, max_genergies, avg_genergies)\n","log_max_avg(data_test_X, max_diff_genergies, avg_diff_genergies)\n","log_max_avg(data_test_X, max_gactivities, avg_gactivities)\n","log_max_avg(data_test_X, max_diff_gactivities, avg_diff_gactivities)"],"metadata":{"id":"DPEPKN5zwQ8B"},"id":"DPEPKN5zwQ8B","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## log_ave_genergy = log(sum / count)"],"metadata":{"id":"m7P0eAW20ewZ"},"id":"m7P0eAW20ewZ"},{"cell_type":"code","source":["def log_sum_count(df, sum_feature, count_feature, save_col_names=0):\n","  new_col_names = []\n","  for i in range (len(sum_feature)):\n","\n","    col_name = ('log__'+df[sum_feature].columns[i][:-2] + \n","                '/' + \n","                df[count_feature].columns[i][:-2]+'_'+str(i+1))\n","    df[col_name] = np.log(df[sum_feature[i]].div(df[count_feature[i]]).fillna(0))\n","    df[col_name].replace([-np.inf, np.inf], 0, inplace=True)\n","\n","\n","    if save_col_names:\n","      new_col_names.append(col_name)\n","\n","  return df, new_col_names"],"metadata":{"id":"B4qCwMedzw2Y"},"id":"B4qCwMedzw2Y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#new series in train dataset\n","data_train_X, sum_count_cols_e2 = log_sum_count(data_train_X, counts_e2, sum_e2, 1)\n","data_train_X, sum_count_cols_e3 = log_sum_count(data_train_X, counts_e3, sum_e3, 1)\n","data_train_X, sum_count_cols_e4 = log_sum_count(data_train_X, counts_e4, sum_e4, 1)\n","data_train_X, sum_count_cols_e5 = log_sum_count(data_train_X, counts_e5, sum_e5, 1)\n","data_train_X, sum_count_cols_e6plus = log_sum_count(data_train_X, counts_e6plus, sum_e6plus, 1)"],"metadata":{"id":"Lt1ZFaF52hBq"},"id":"Lt1ZFaF52hBq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#same new series in test dataset\n","log_sum_count(data_test_X, counts_e2, sum_e2)\n","log_sum_count(data_test_X, counts_e3, sum_e3)\n","log_sum_count(data_test_X, counts_e4, sum_e4)\n","log_sum_count(data_test_X, counts_e5, sum_e5)\n","log_sum_count(data_test_X, counts_e6plus, sum_e6plus)"],"metadata":{"id":"WbPkAquE97gE"},"id":"WbPkAquE97gE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistics computing"],"metadata":{"id":"x2TbblLC-b_-"},"id":"x2TbblLC-b_-"},{"cell_type":"markdown","source":["* minimum\n","* maximum\n","* std\n","* quantiles: 0,25 ; 0.5 ; 0.75\n","* information if there is non-zero value in the test series (True/ False),\n","* number of positive values in a series,\n","* hours from last non-zero measurement,\n","* number of times a series increased in comparison to the previous hour's recording, \n","\n"],"metadata":{"id":"RLDXwXXw_gUJ"},"id":"RLDXwXXw_gUJ"},{"cell_type":"code","source":["def last_nonzero_idx(x):\n","  return (len(x) - np.where(x != 0)[0][-1]-1) if any(x != 0) else len(x)+1"],"metadata":{"id":"L2pC-FvDMvvN"},"id":"L2pC-FvDMvvN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def num_times_increased(feature):\n","  counter = 0\n","  for i in range(1, len(feature)):\n","    if (feature[i] - feature[i-1]) > 0:\n","      counter += 1\n","  return counter"],"metadata":{"id":"J6X01C0Dl1-_"},"id":"J6X01C0Dl1-_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["ts_1 = [sum_count_cols_e2, sum_count_cols_e3, sum_count_cols_e4, sum_count_cols_e5,\n","        sum_count_cols_e6plus, num_rock_bursts, h_bump_energy]\n","ts_2 = [log_max_avg_genergies, log_diff_max_avg_genergies, \n","        log_max_avg_gactivities, log_diff_max_avg_gactivities] "],"metadata":{"id":"q_0nL2QN7r8_"},"id":"q_0nL2QN7r8_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ts_compute_stats(df, ts, abs, last_X_hours):\n","    \n","    col_name = ts[0][:-2]\n","\n","    for hour in last_X_hours:\n","\n","      #min\n","      df[f'{hour}_min_'+ col_name] = df[ts[-hour: ]].min(axis=1)\n","      #max\n","      df[f'{hour}_max_'+ col_name] = df[ts[-hour: ]].max(axis=1)\n","      #std\n","      df[f'{hour}_std_'+ col_name] = df[ts[-hour: ]].std(axis=1)\n","      #quantiles\n","      df[f'{hour}_q0.25_'+ col_name] = df[ts[-hour: ]].quantile(q=0.25, axis=1)\n","      df[f'{hour}_q0.5_'+ col_name] = df[ts[-hour: ]].quantile(q=0.5, axis=1)\n","      df[f'{hour}_q0.75_'+ col_name] = df[ts[-hour: ]].quantile(q=0.75, axis=1)\n","      #number of times a series increased in comparison to the previous hour's recording\n","      df[f'{hour}_num_incr_' + col_name] = df[ts[-hour: ]].apply(\n","          num_times_increased,\n","           axis=1\n","           )\n","      #number of positive values in the series\n","      df[f'{hour}_num_pos_'] = df[df[ts[-hour: ]]>0][ts].count(axis=1)\n","\n","      if abs:\n","        #abs_max\n","        df[f'{hour}_max_abs_'+ col_name] = df[ts[-hour: ]].abs().max(axis=1)\n","      else:\n","        #hours from last non-zero observation\n","        df[f'{hour}_h_from_l_nonzero_' + col_name] = df[ts[-hour: ]].apply(\n","            last_nonzero_idx,\n","             axis=1\n","             )\n","\n","    return df"],"metadata":{"id":"KXz4uquq_Iic"},"id":"KXz4uquq_Iic","execution_count":null,"outputs":[]},{"cell_type":"code","source":["hours = [4, 8, 24]\n","#computing stats for X_train without features for which no ABS calcutation is\n","#needed\n","for ts in ts_1:\n","  ts_compute_stats(data_train_X, ts, False, hours)"],"metadata":{"id":"alRWCPoA1usm"},"id":"alRWCPoA1usm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#computing stats for X_train without features for which ABS calcutation is\n","#needed\n","for ts in ts_2:\n","  ts_compute_stats(data_train_X, ts, True, hours)"],"metadata":{"id":"S3bUD94TtZrJ"},"id":"S3bUD94TtZrJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#computing stats for X_test without features for which no ABS calcutation is\n","#needed\n","for ts in ts_1:\n","  ts_compute_stats(data_test_X, ts, False, hours)"],"metadata":{"id":"KETR8Qp8wTiB"},"id":"KETR8Qp8wTiB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#computing stats for X_test without features for which ABS calcutation is\n","#needed\n","for ts in ts_2:\n","  ts_compute_stats(data_test_X, ts, False, hours)"],"metadata":{"id":"QPvHPq4RwWeo"},"id":"QPvHPq4RwWeo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Two features from working_site_metadata derived to base dataframe"],"metadata":{"id":"IdJ0pBNw5Qqj"},"id":"IdJ0pBNw5Qqj"},{"cell_type":"code","source":["def find_height(x, dictionary):\n","    for key in dictionary:\n","        if x == key:\n","            return (dictionary[key][0])"],"metadata":{"id":"yiM-C83o5dvl"},"id":"yiM-C83o5dvl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_h_assessment(x, dictionary):\n","    for key in dictionary:\n","        if x == key:\n","            return (dictionary[key][1])"],"metadata":{"id":"yYlxEO4L3BQe"},"id":"yYlxEO4L3BQe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict_id = {}\n","for i, row in working_site_data.iterrows():\n","    dict_id[row['main_working_id']] = (row['main_working_height'], row['mining_hazard_assessment'])"],"metadata":{"id":"VY2gRX6a5nH8"},"id":"VY2gRX6a5nH8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train_X['height'] = data_train_X['main_working_id'].apply(lambda x: find_height(x, dict_id))\n","data_test_X['height'] = data_test_X['main_working_id'].apply(lambda x: find_height(x, dict_id))"],"metadata":{"id":"8dp8wGsO5sxE"},"id":"8dp8wGsO5sxE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train_X['mining_hazard_assessment'] = data_train_X['main_working_id'].apply(lambda x: find_h_assessment(x, dict_id))\n","data_test_X['mining_hazard_assessment'] = data_test_X['main_working_id'].apply(lambda x: find_h_assessment(x, dict_id))"],"metadata":{"id":"eMX_wja35wY3"},"id":"eMX_wja35wY3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Categorical variables"],"metadata":{"id":"_PBFjFQw6Fzx"},"id":"_PBFjFQw6Fzx"},{"cell_type":"markdown","id":"d84e4cb5","metadata":{"id":"d84e4cb5"},"source":["latest_seismoacoustic_assessment "]},{"cell_type":"code","execution_count":null,"id":"247ec946","metadata":{"id":"247ec946"},"outputs":[],"source":["data_train_X['latest_seismoacoustic_assessment'].value_counts()"]},{"cell_type":"code","execution_count":null,"id":"b02b18ee","metadata":{"id":"b02b18ee"},"outputs":[],"source":["data_train_X['latest_seismoacoustic_assessment'].replace('d', 'c', inplace=True)\n","#'d' category contains only 48 instances. What's even more important, in the test data there is no category 'd'.\n","#'d' category will be replaced to 'c'."]},{"cell_type":"code","source":["data_train_X = pd.get_dummies(data_train_X, drop_first=True)"],"metadata":{"id":"AnOva94x57rZ"},"id":"AnOva94x57rZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_test_X = pd.get_dummies(data_test_X, drop_first=True)"],"metadata":{"id":"pAPPy0vJ6eFc"},"id":"pAPPy0vJ6eFc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_test_X = data_test_X.reindex(columns = data_train_X.columns, fill_value=0)"],"metadata":{"id":"cwmUlRvz6O5d"},"id":"cwmUlRvz6O5d","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Removing features"],"metadata":{"id":"AX9HvzEONqzj"},"id":"AX9HvzEONqzj"},{"cell_type":"code","source":["import itertools"],"metadata":{"id":"gsQLCCqTZiPb"},"id":"gsQLCCqTZiPb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing features that were used to time series transformation\n","ts_to_remove = list(itertools.chain.from_iterable(ts_to_agg))"],"metadata":{"id":"uvHVrmW7aRwC"},"id":"uvHVrmW7aRwC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train_X.drop(ts_to_remove, axis=1, inplace=True)\n","data_test_X.drop(ts_to_remove, axis=1, inplace=True)"],"metadata":{"id":"5i0BO1AZafep"},"id":"5i0BO1AZafep","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#main_working_id will be dropped\n","data_train_X.drop('main_working_id', axis=1, inplace=True)\n","data_test_X.drop('main_working_id', axis=1, inplace=True)"],"metadata":{"id":"bRgllMA9bZTD"},"id":"bRgllMA9bZTD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#also total_number_of bumps will be dropped, as it's exactly the same information as summed\n","#counts of bumps(count_e*), they're just summed values from all detectors \n","data_train_X.drop(total_number_of_bumps, axis=1, inplace=True)\n","data_test_X.drop(total_number_of_bumps, axis=1, inplace=True)"],"metadata":{"id":"A-cgheQ4mtL0"},"id":"A-cgheQ4mtL0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New dataset shape"],"metadata":{"id":"xmdQSuEPMxUr"},"id":"xmdQSuEPMxUr"},{"cell_type":"code","source":["data_train_X.shape, data_test_X.shape"],"metadata":{"id":"xfxZ4K9IvQrk"},"id":"xfxZ4K9IvQrk","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e570eb1e","metadata":{"id":"e570eb1e"},"outputs":[],"source":["#storing variables\n","data_train_X.to_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/EXTRACTED DATA/X_train3', index=False)\n","data_test_X.to_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/EXTRACTED DATA/X_test3', index=False)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"private_outputs":true,"provenance":[{"file_id":"11Sd3ZJHCxfSo6QGHR6SEN-NlaoWEK886","timestamp":1679674906255}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}