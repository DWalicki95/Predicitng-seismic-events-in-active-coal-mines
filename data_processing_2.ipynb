{"cells":[{"cell_type":"markdown","id":"7f8849bd","metadata":{"id":"7f8849bd"},"source":["# Data Processing"]},{"cell_type":"code","execution_count":null,"id":"a654961c","metadata":{"id":"a654961c"},"outputs":[],"source":["from IPython.display import display, HTML\n","display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import random"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"V5b7fpXLlH0J"},"id":"V5b7fpXLlH0J","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"233f343b","metadata":{"id":"233f343b"},"outputs":[],"source":["data_train_X = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_train_X')\n","data_train_y = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_train_y')\n","data_test_X = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_test_X')\n","data_test_y = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/input_data_test_y')\n","working_site_data = pd.read_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/DATA/working_site_metadata.csv')"]},{"cell_type":"code","execution_count":null,"id":"6fc5ee5a","metadata":{"id":"6fc5ee5a"},"outputs":[],"source":["data_train_X.shape"]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"id":"F4bJB57fipO2"},"id":"F4bJB57fipO2"},{"cell_type":"markdown","id":"68169a17","metadata":{"id":"68169a17"},"source":["This is second version of dataset processing (second version of final dataset). \n","\n","\n","The main idea was to extract features like:\n","* minimum\n","* maximum\n","* std\n","* information if there is non-zero value in the test series (True/ False),\n","* hours from last non-zero measurement.\n","These statistic were computed over the last 2, 4, 8 and 24 hours. \n","\n","Above extraction methodology affect following time series:\n","* count_e, \n","* sum_e, \n","* number_of_rock_bursts\n","* number_of_destressing_blasts\n","* avg_gactivity\n","* avg_genergy\n","\n","For series:\n","* avg_difference_in_gactivity \n","* avg_difference_in_genergy \n","computed max abs value over last 2 and 24 hours.\n","\n","Categorical variables:\n","* latest_seismic_assesment\n","* latest_seismoacoustic_assessment \n","* latest_comprehensive_assessment\n","* latetest_hazards_assessment\n","* mining_hazard_assessment (from working_site_metada additional dataset)\n","were encoded with one-hot encoding.\n","\n","---------------------------------------------\n","From extra dataset following features were used:\n","* mining_hazard_assessment (as mentioned above)\n","* main_working_height \n","---------------------------------------------\n","Time series describing maximum statistics were <b>dropped</b> from dataset, as they are probably highly correlated with average statistics:\n","* max_gactivity.X\n","* max_genergy.X\n","* max_difference_in_gactivity.X\n","* max_difference_in_genergy.X"]},{"cell_type":"code","execution_count":null,"id":"c6c149f1","metadata":{"id":"c6c149f1"},"outputs":[],"source":["def max_nonzero_index(row):\n","    non_zero_cols = row.nonzero()[0]\n","    \n","    if len(non_zero_cols) == 0:\n","        return -1\n","    else:\n","        return non_zero_cols[-1]"]},{"cell_type":"code","source":["def features_extracting(data, windows_size, coord1, coord2):\n","    coords = np.arange(coord1, coord2+1, 24)\n","    idx = 0\n","    df_1 = pd.DataFrame()\n","    df_add = pd.DataFrame()\n","    while idx < len(coords)-1:\n","         \n","        for window in windows_size:\n","        \n","            base_col_name = data.iloc[:, coords[idx+1]-window: coords[idx+1]].columns[0][:-2]\n","            max_nonzero_idx = []\n","\n","            #min\n","            df_add[str(window)+'h_min_'+base_col_name] = data.iloc[:, coords[idx+1]-window: coords[idx+1]].min(axis=1)\n","\n","\n","            #max\n","            df_add[str(window)+'h_max_'+base_col_name] = data.iloc[:, coords[idx+1]-window: coords[idx+1]].max(axis=1)\n","\n","            #std\n","            df_add[str(window)+'h_std_'+base_col_name] = data.iloc[:, coords[idx+1]-window: coords[idx+1]].std(axis=1)\n","\n","            #if there is a non-zero value in the test series\n","            df_add[str(window)+'h_nonzero_'+base_col_name] = data.iloc[:, coords[idx+1]-window: coords[idx+1]].any(axis=1)\n","\n","            #how many hours from the last non-zero observation\n","            for i, row in data.iloc[:, coords[idx]: coords[idx+1]].iterrows():\n","                max_idx = 25\n","                for j in range(len(row)):\n","                    if row[j] != 0:\n","                        max_idx = 24-(j+1)\n","                max_nonzero_idx.append(max_idx)   \n","            df_add['last_non_0_'+base_col_name] = max_nonzero_idx\n","\n","        idx += 1\n","        \n","    return df_add"],"metadata":{"id":"2e6fa7be"},"id":"2e6fa7be","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"99f1f371","metadata":{"id":"99f1f371"},"outputs":[],"source":["def max_abs_value(data, windows_size, coord1, coord2):\n","    coords = np.arange(coord1, coord2+1, 24)\n","    idx = 0\n","    df_1 = pd.DataFrame()\n","    df_add = pd.DataFrame()\n","    while idx < len(coords)-1:\n","         \n","        for window in windows_size:\n","            base_col_name = data.iloc[:, coords[idx+1]-window: coords[idx+1]].columns[0][:-2]\n","            #max_abs_value\n","            df_add[str(window)+'h_max_abs_val'+base_col_name] = data.iloc[:, coords[idx+1]-window: coords[idx+1]].abs().max(axis=1)\n","\n","        idx += 1\n","        \n","    return df_add"]},{"cell_type":"code","execution_count":null,"id":"b99acc45","metadata":{"id":"b99acc45"},"outputs":[],"source":["df1_train = features_extracting(data_train_X, (24, 8, 4, 2), 13, 252)\n","df1_test = features_extracting(data_test_X, (24, 8, 4, 2), 13, 252)\n","df2_train = features_extracting(data_train_X, (24, 8, 4, 2), 276, 324)\n","df2_test = features_extracting(data_test_X, (24, 8, 4, 2), 276, 324)"]},{"cell_type":"code","execution_count":null,"id":"295d694d","metadata":{"id":"295d694d"},"outputs":[],"source":["df3_abs = max_abs_value(data_train_X, (24, 2), 397, 445)\n","df3_test_abs = max_abs_value(data_test_X, (24, 2), 397, 445)"]},{"cell_type":"code","execution_count":null,"id":"8e0f3a8e","metadata":{"id":"8e0f3a8e"},"outputs":[],"source":["df_train = pd.concat([data_train_X, df1_train, df2_train, df3_abs], axis=1)\n","df_test = pd.concat([data_test_X, df1_test, df2_test, df3_test_abs], axis=1)"]},{"cell_type":"markdown","source":["Adding two features from extra dataset."],"metadata":{"id":"wcAYvPIXvJmd"},"id":"wcAYvPIXvJmd"},{"cell_type":"code","execution_count":null,"id":"5755b8c8","metadata":{"id":"5755b8c8"},"outputs":[],"source":["def find_height(x, dictionary):\n","    for key in dictionary:\n","        if x == key:\n","            return (dictionary[key][0])"]},{"cell_type":"code","execution_count":null,"id":"71859065","metadata":{"id":"71859065"},"outputs":[],"source":["def find_h_assessment(x, dictionary):\n","    for key in dictionary:\n","        if x == key:\n","            return (dictionary[key][1])"]},{"cell_type":"code","execution_count":null,"id":"d7ec5fe6","metadata":{"id":"d7ec5fe6"},"outputs":[],"source":["dict_id = {}\n","for i, row in working_site_data.iterrows():\n","    dict_id[row['main_working_id']] = (row['main_working_height'], row['mining_hazard_assessment'])"]},{"cell_type":"code","execution_count":null,"id":"3ad317f7","metadata":{"id":"3ad317f7"},"outputs":[],"source":["df_train['height'] = df_train['main_working_id'].apply(lambda x: find_height(x, dict_id))\n","df_test['height'] = df_test['main_working_id'].apply(lambda x: find_height(x, dict_id))"]},{"cell_type":"code","execution_count":null,"id":"8b556a44","metadata":{"id":"8b556a44"},"outputs":[],"source":["df_train['mining_hazard_assessment'] = df_train['main_working_id'].apply(lambda x: find_h_assessment(x, dict_id))\n","df_test['mining_hazard_assessment'] = df_test['main_working_id'].apply(lambda x: find_h_assessment(x, dict_id))"]},{"cell_type":"markdown","id":"73c6e086","metadata":{"id":"73c6e086"},"source":["# Dropping features"]},{"cell_type":"code","source":["#firstly - main working id from general features\n","df_train.drop(['main_working_id'], inplace=True, axis=1)\n","df_test.drop(['main_working_id'], inplace=True, axis=1)"],"metadata":{"id":"9jfsHxza1n9U"},"id":"9jfsHxza1n9U","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing time series used for aggregation\n","df_train.drop(df_train.iloc[:, 12: 252], inplace=True, axis=1)\n","df_test.drop(df_test.iloc[:, 12: 252], inplace=True, axis=1)"],"metadata":{"id":"IYOTVTYI5SXA"},"id":"IYOTVTYI5SXA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#removing time series used for aggregation\n","df_train.drop(df_train.iloc[:, 36: 84], inplace=True, axis=1)\n","df_test.drop(df_test.iloc[:, 36: 84], inplace=True, axis=1)"],"metadata":{"id":"53q7-JNM_CD8"},"id":"53q7-JNM_CD8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dropping maximum statistics\n","df_train.drop(df_train.iloc[:, 60: 108], inplace=True, axis=1) #max_gactivity and max_genrgy \n","df_test.drop(df_test.iloc[:, 60: 108], inplace=True, axis=1)"],"metadata":{"id":"Zz5ekIth_lcE"},"id":"Zz5ekIth_lcE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.drop(df_train.iloc[:, 108: 156], inplace=True, axis=1) #max_difference_in_gactivity and max_difference_in_genrgy\n","df_test.drop(df_test.iloc[:, 108: 156], inplace=True, axis=1)"],"metadata":{"id":"IWwK3Ce3AdL3"},"id":"IWwK3Ce3AdL3","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d16c16b1","metadata":{"id":"d16c16b1"},"outputs":[],"source":["df_train = pd.get_dummies(df_train, drop_first=True)\n","df_test = pd.get_dummies(df_test, drop_first = True)\n","df_test = df_test.reindex(columns = df_train.columns, fill_value=0)"]},{"cell_type":"code","execution_count":null,"id":"e570eb1e","metadata":{"id":"e570eb1e"},"outputs":[],"source":["#storing variables\n","df_train.to_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/EXTRACTED DATA/X_train_v2', index=False)\n","df_test.to_csv('/content/drive/MyDrive/Data Science Pro/Projekt końcowy/EXTRACTED DATA/X_test_v2', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"private_outputs":true,"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}